{
    "basic_info": {
      "description_header": "Hello!",
      "description": "ðŸ‘‹ Hello, I'm Mandela Steele-Dadzie! With a rich background spanning over six years in strategy development and cross-functional collaboration, I've recently broadened my skill set to include advanced proficiency in Python and JavaScript through a coding bootcamp. Now, I am eagerly exploring opportunities in analytics, data science, partnership management, and strategy. My mission is to harness my analytical skills and technical expertise to uncover data-driven insights and elevate operational excellence in diverse business environments. Beyond work, you'll find me exploring new destinations, cherishing moments with family, and embarking on invigorating hikes with my pawed pal, Dragonflyâ€”a Labrador/Boxer mix with energy to spare!",
      "section_name": {
        "about": "About me",
        "projects": "Projects",
        "skills": "Skills",
        "experience": "Experience"
      }
    },
    "projects": [
      {
        "title": "Yelp Customer Review Data Stream",
        "startDate": "2023",
        "description": "Real-time Yelp Customer Review Dataset data stream using Apache Spark and Apache Kafka, incorporating NLTK large language model for sentiment analysis",
        "images": [
          "images/portfolio/yelp_socket_stream/yelp_socket_spark_2.png",
          "images/portfolio/yelp_socket_stream/yelp_socket_confluent_topic.png",
          "images/portfolio/yelp_socket_stream/yelp_socket_confluent_connector.png",
          "images/portfolio/yelp_socket_stream/yelp_socket_elasticsearch_index.png",
          "images/portfolio/yelp_socket_stream/yelp_socket_elasticsearch_queries.png"
        ],
        "url": "https://github.com/msteeledadzie422/yelp_socket_stream",
        "technologies": [
          {
            "class": "devicon-apache-plain",
            "name": "Apache Spark"
          },
          {
            "class": "devicon-apachekafka-original",
            "name": "Microsoft Azure"
          },
          {
            "class": "devicon-docker-plain",
            "name": "Docker"
          }
        ]
      },
      {
        "title": "Global Stadiums Data Visualization",
        "startDate": "2023",
        "description": "List of global stadiums pulled from Wikipedia. Data pipeline created using Apache Airflow and Azure Data Lakes",
        "images": [
          "images/portfolio/wikipedia_stadiums/wikipedia_airflow_graph.png",
          "images/portfolio/wikipedia_stadiums/wikipedia_azure_synapse.png",
          "images/portfolio/wikipedia_stadiums/wikipedia_tableau_1.png",
          "images/portfolio/wikipedia_stadiums/wikipedia_tableau_2.png"
        ],
        "url": "https://github.com/msteeledadzie422/wikipedia_soccer_stadium_data_project",
        "technologies": [
          {
            "class": "devicon-apache-plain",
            "name": "Apache Airflow"
          },
          {
            "class": "devicon-azure-plain",
            "name": "Microsoft Azure"
          },
          {
            "class": "devicon-docker-plain",
            "name": "Docker"
          },
          {
            "class": "devicon-pandas-plain",
            "name": "Pandas"
          }
        ]
      },
      {
        "title": "UK Retail Data Pipeline",
        "startDate": "2023",
        "description": "End-to-end Apache Airflow data pipeline set up using Astro CLI",
        "images": [
          "images/portfolio/uk_retail_pipeline/airflow_img_1.png",
          "images/portfolio/uk_retail_pipeline/airflow_img_2.png",
          "images/portfolio/uk_retail_pipeline/airflow_img_3.png",
          "images/portfolio/uk_retail_pipeline/gcs_bucket.png",
          "images/portfolio/uk_retail_pipeline/bigquery_tables.png",
          "images/portfolio/uk_retail_pipeline/metadata_visualization.png"
        ],
        "url": "https://github.com/msteeledadzie422/airflow_retail_data",
        "technologies": [
          {
            "class": "devicon-apache-plain",
            "name": "Apache Airflow"
          },
          {
            "class": "devicon-googlecloud-plain",
            "name": "Google Cloud Services"
          }
        ]
      },
      {
        "title": "Zillow Rapid API Data Pipeline",
        "startDate": "2023",
        "description": "End-to-end data pipeline hosted on an AWS EC2 Instance that uses Airflow DAGS to pull proxy Zillow real estate data from Rapid API",
        "images": [
          "images/portfolio/zillow_rapidapi/zillow_airflow_img.png",
          "images/portfolio/zillow_rapidapi/zillow_s3_img.png",
          "images/portfolio/zillow_rapidapi/zillow_redshift_img.png",
          "images/portfolio/zillow_rapidapi/zillow_quicksight_img.png"
        ],
        "url": "https://github.com/msteeledadzie422/zillow_rapidapi_data_pipeline",
        "technologies": [
          {
            "class": "devicon-apache-plain",
            "name": "Apache Airflow"
          },
          {
            "class": "devicon-amazonwebservices-original",
            "name": "Amazon Web Services"
          }
        ]
      },
      {
        "title": "2020 Tokyo Olympics Data Pipeline",
        "startDate": "2023",
        "description": "This project is an end-to-end data pipeline that pulls 2020 Tokyo Olympics data stored in GitHub into Microsoft Azure via an Azure Data Factory, with ultimate analysis completed in Azure Synapse",
        "images": [
          "images/portfolio/azure_tokyo_olympics/tokyo_olympics_data_factory.png",
          "images/portfolio/azure_tokyo_olympics/tokyo_olympics_data_container.png",
          "images/portfolio/azure_tokyo_olympics/tokyo_olympics_databricks.png",
          "images/portfolio/azure_tokyo_olympics/tokyo_olympics_synapse.png"
        ],
        "url": "https://github.com/msteeledadzie422/msd-azure-tokyo-olympics-data",
        "technologies": [
          {
            "class": "devicon-azure-plain",
            "name": "Microsoft Azure"
          }
        ]
      },
      {
        "title": "Redfin City Real Estate Data Pipeline",
        "startDate": "2023",
        "description": "An ETL data engineering project that uses Apache Airflow, Snowpipe, Snowflake, and AWS Services to extract real estate data from Redfin.",
        "images": [
          "images/portfolio/redfin_city_data/redfin_airflow_dags.png",
          "images/portfolio/redfin_city_data/redfin_s3_bucket_raw.png",
          "images/portfolio/redfin_city_data/redfin_s3_bucket_transformed.png",
          "images/portfolio/redfin_city_data/redfin_snowflake.png"
        ],
        "url": "https://github.com/msteeledadzie422/redfin_data_pipeline",
        "technologies": [
          {
            "class": "devicon-apache-plain",
            "name": "Apache Airflow"
          },
          {
            "class": "devicon-amazonwebservices-original",
            "name": "Amazon Web Services"
          }
        ]
      }
    ],
    "experience": [
      {
        "company": "Code Fellows",
        "title": "Software Development Bootcamp",
        "years": "05.2022 - 05.2023",
        "mainTech": [
          
        ],
        "technologies": [

        ]
      },
      {
        "company": "DoorDash",
        "title": "Enterprise Partnerships",
        "years": "05.2019 - 05.2022",
        "mainTech": [
          
        ],
        "technologies": [

        ]
      },
      {
        "company": "Sentieo, Inc. (an AlphaSense Company)",
        "title": "Sales Development",
        "years": "05.2018 - 05.2019",
        "mainTech": [
          
        ],
        "technologies": [

        ]
      },
      {
        "company": "Flexport Inc",
        "title": "Global Pricing",
        "years": "10.2016 - 05.2018",
        "mainTech": [
        
        ],
        "technologies": [

        ]
      }
    ]
  }
